# PydanticAI Workflow System - Environment Configuration
# Copy this file to .env and update with your values
# Supports 40+ AWS Bedrock models with automatic fallback

# Database Configuration
REDIS_URL=redis://localhost:6379
POSTGRES_URL=postgresql://postgres:password@localhost:5432/pydantic_ai

# AWS Bedrock Configuration (Primary AI Service)
AWS_ACCESS_KEY_ID=your_aws_access_key_id
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key
AWS_DEFAULT_REGION=us-east-1
AWS_SESSION_TOKEN=your_session_token_if_using_temp_credentials

# Bedrock Model Configuration (40+ Models Supported)
# Embedding Models: amazon.titan-embed-text-v1, amazon.titan-embed-text-v2:0, cohere.embed-english-v3, etc.
AWS__BEDROCK_MODEL_ID=amazon.titan-embed-text-v1
# Chat Models: claude-3-sonnet, claude-3-haiku, amazon.nova-pro-v1:0, meta.llama3-1-405b, etc.
AWS__BEDROCK_CHAT_MODEL=anthropic.claude-3-haiku-20240307-v1:0
AWS__BEDROCK_EMBEDDING_DIMENSION=1536

# OpenAI API (optional - fallback)
OPENAI_API_KEY=your_openai_api_key_here

# MCP Server Configuration (YAML-Based Multi-Server Support)
MCP__CONFIG_FILE=./config/mcp_servers.yaml
MCP__AUTO_RELOAD=true
MCP__CONNECTION_POOL_SIZE=10

# Application Settings
DEBUG=true
LOG_LEVEL=INFO

# Embedding Configuration
# Primary: Bedrock embeddings with 40+ model support
# Fallback: Sentence-transformers for development
EMBEDDING__BEDROCK_MODEL=amazon.titan-embed-text-v1
EMBEDDING__FALLBACK_MODEL=all-MiniLM-L6-v2
EMBEDDING__FALLBACK_DIMENSION=384
EMBEDDING__MODEL_NAME=bedrock
EMBEDDING__CACHE_EMBEDDINGS=true

# Chunking Configuration
CHUNKING__DEFAULT_CHUNK_SIZE=1000
CHUNKING__DEFAULT_OVERLAP=100
CHUNKING__MAX_CHUNK_SIZE=10000
CHUNKING__MIN_CHUNK_SIZE=100

# Memory Configuration
MEMORY__MAX_SHORT_TERM_CHUNKS=50
MEMORY__MAX_CONTEXT_TOKENS=2000
MEMORY__CONSOLIDATION_INTERVAL=3600
MEMORY__IMPORTANCE_THRESHOLD=0.5

# Storage Settings
FAISS_INDEX_PATH=./data/faiss_index
VECTOR_DIMENSION=1536

# Optional: OpenAI API (fallback support)
OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# MODEL CONFIGURATION EXAMPLES
# =============================================================================

# Example Bedrock Embedding Models (choose one):
# EMBEDDING__BEDROCK_MODEL=amazon.titan-embed-text-v1          # 1536 dimensions
# EMBEDDING__BEDROCK_MODEL=amazon.titan-embed-text-v2:0        # 1024 dimensions  
# EMBEDDING__BEDROCK_MODEL=cohere.embed-english-v3             # 1024 dimensions
# EMBEDDING__BEDROCK_MODEL=cohere.embed-multilingual-v3        # 1024 dimensions

# Example Bedrock Chat Models (choose one):
# AWS__BEDROCK_CHAT_MODEL=anthropic.claude-3-sonnet-20240229-v1:0     # Claude 3 Sonnet
# AWS__BEDROCK_CHAT_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0   # Claude 3.5 Sonnet
# AWS__BEDROCK_CHAT_MODEL=amazon.nova-pro-v1:0                        # Amazon Nova Pro
# AWS__BEDROCK_CHAT_MODEL=meta.llama3-1-405b-instruct-v1:0           # Meta Llama 3.1 405B
# AWS__BEDROCK_CHAT_MODEL=mistral.mistral-large-2402-v1:0            # Mistral Large

# =============================================================================
# MCP SERVER EXAMPLES (configured in config/mcp_servers.yaml)
# =============================================================================

# The following MCP servers are pre-configured and can be enabled in mcp_servers.yaml:
# - filesystem: File operations with uvx mcp-server-filesystem
# - git: Version control with uvx mcp-server-git  
# - brave-search: Web search with uvx mcp-server-brave-search
# - postgres: Database operations with uvx mcp-server-postgres
# - sqlite: Local database with uvx mcp-server-sqlite

# To add your own API keys for MCP servers, create a .env file and add:
# BRAVE_API_KEY=your_brave_search_api_key_here
# DATABASE_URL=postgresql://user:password@localhost/dbname

# =============================================================================
# DEVELOPMENT vs PRODUCTION CONFIGURATION
# =============================================================================

# DEVELOPMENT (no AWS credentials required):
# - System automatically falls back to sentence-transformers embeddings
# - Uses local models for development and testing
# - All MCP servers work without cloud dependencies

# PRODUCTION (with AWS Bedrock):
# - Configure AWS credentials above
# - System uses Bedrock embeddings and chat models
# - Enhanced performance with cloud-scale AI models
# - Automatic fallback if Bedrock temporarily unavailable
